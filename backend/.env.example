# CXPM AI PRD Backend Configuration
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# ===========================================
# LLM Configuration
# ===========================================

# Circuit API (Cisco's AI platform - primary provider)
# The {model} placeholder in the URL will be replaced with CIRCUIT_MODEL
CIRCUIT_BASE_URL=https://chat-ai.cisco.com/openai/deployments/{model}/chat/completions
CIRCUIT_MODEL=gpt-4

# Ollama (local LLM - alternative provider)
# Make sure Ollama is running: ollama serve
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Claude API (cloud LLM - fallback when Ollama is unavailable)
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# ===========================================
# Database Configuration
# ===========================================

# SQLite database path (default)
DATABASE_URL=sqlite:///./cxpm.db

# ===========================================
# Application Settings
# ===========================================

# Maximum file upload size in KB
MAX_FILE_SIZE_KB=50

# LLM request timeout in seconds
LLM_TIMEOUT=60

# Maximum retry attempts for LLM calls
LLM_MAX_RETRIES=1

# Text chunk size for processing long documents (in characters)
CHUNK_SIZE_CHARS=4000
